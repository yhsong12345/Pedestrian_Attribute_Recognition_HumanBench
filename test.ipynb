{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-24T07:52:48.699316Z",
     "start_time": "2025-02-24T07:52:46.826564Z"
    }
   },
   "source": [
    "import os\n",
    "import cv2\n",
    "import pickle\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "device ='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(device)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T07:52:48.728595Z",
     "start_time": "2025-02-24T07:52:48.725569Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pickle_path = '/purestorage/AILAB/AI_2/youhans/workspace/reid/person/HumanBench/PA100k/dataset_all.pkl'\n",
    "root_path = \"/purestorage/AILAB/AI_2/datasets/ReID/PA-100k/data/release_data/PA-100k/\""
   ],
   "id": "7602898876dda56e",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T07:52:49.353281Z",
     "start_time": "2025-02-24T07:52:49.173405Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pickle_file = pickle.load(open(pickle_path, 'rb'))\n",
    "print(pickle_file.keys())\n",
    "\n",
    "image_names = pickle_file['image_name']\n",
    "labels = pickle_file['label']"
   ],
   "id": "97723e810fa4c6e3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['description', 'reorder', 'root', 'image_name', 'label', 'attr_name', 'label_idx', 'partition', 'weight_train', 'weight_trainval'])\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T08:16:45.122635Z",
     "start_time": "2025-02-24T08:16:45.107706Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch.utils.data as data\n",
    "import cv2\n",
    "# from petrelbox.io import PetrelHelper\n",
    "from PATH.core.data.transforms.pedattr_transforms import PedAttrAugmentation, PedAttrTestAugmentation, PedAttrRandomAugmentation\n",
    "\n",
    "\n",
    "class AttrDataset(data.Dataset):\n",
    "\n",
    "    def __init__(self, ginfo, augmentation, task_spec, train=True, data_use_ratio=1, **kwargs):\n",
    "\n",
    "        # assert task_spec.dataset in ['peta', 'PA-100k', 'rap', 'rap2', 'uavhuman', 'HARDHC', 'ClothingAttribute', 'parse27k', 'duke', 'market'], \\\n",
    "        #     f'dataset name {task_spec.dataset} is not exist'\n",
    "\n",
    "        data_path = \"/purestorage/AILAB/AI_2/youhans/workspace/reid/person/HumanBench/PA100k/dataset_all.pkl\"\n",
    "        # dataset_info = PetrelHelper.pickle_load(data_path)\n",
    "        dataset_info = pickle.load(open(data_path, 'rb+'))\n",
    "\n",
    "\n",
    "        img_id = dataset_info[\"image_name\"]\n",
    "        attr_label = dataset_info[\"label\"]\n",
    "        attr_label[attr_label == 2] = 0\n",
    "\n",
    "        if train:\n",
    "            split = 'trainval'\n",
    "        else:\n",
    "            split = 'test'\n",
    "\n",
    "        assert split in dataset_info[\"partition\"].keys(), f'split {split} is not exist'\n",
    "\n",
    "        height = 256\n",
    "        width = 192\n",
    "\n",
    "        # self.dataset = \"/purestorage/AILAB/AI_2/youhans/workspace/reid/person/HumanBench/PA100k/dataset_all.pkl\"\n",
    "        self.root_path = '/purestorage/AILAB/AI_2/datasets/ReID/PA-100k/data/release_data/PA-100k'\n",
    "\n",
    "        if train:\n",
    "            self.transform = PedAttrAugmentation(height, width)\n",
    "            self.transform = PedAttrRandomAugmentation(height, width, \\\n",
    "                10, 10)\n",
    "        else:\n",
    "            self.transform = PedAttrTestAugmentation(height, width)\n",
    "\n",
    "        self.attr_id = dataset_info[\"attr_name\"]\n",
    "        self.attr_num = len(self.attr_id)\n",
    "\n",
    "        self.img_idx = dataset_info[\"partition\"][split]\n",
    "\n",
    "        # if isinstance(self.img_idx, list):\n",
    "        #     self.img_idx = self.img_idx[0]  # default partition 0\n",
    "        #\n",
    "        # if data_use_ratio != 1:\n",
    "        #     self.img_idx = random.sample(list(self.img_idx), int(len(self.img_idx) * data_use_ratio))\n",
    "\n",
    "        # self.img_num = len(self.img_idx)\n",
    "        self.img_num = self.img_idx.shape[0]\n",
    "        # self.img_idx = np.array(self.img_idx)\n",
    "\n",
    "        self.img_id = [img_id[i] for i in self.img_idx]\n",
    "        self.label = attr_label[self.img_idx]\n",
    "        # self.task_name = ginfo.task_name\n",
    "\n",
    "\n",
    "    # def __getitem__(self, index):\n",
    "    #     return self.read_one(index)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        print(index)\n",
    "        imgname = self.img_id[index]\n",
    "        gt_label = self.label[index]\n",
    "        img_path = os.path.join(self.root_path, imgname)\n",
    "\n",
    "\n",
    "        img = Image.open(img_path)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        gt_label = gt_label.astype(np.float32)\n",
    "        #\n",
    "        # img = torch.from_numpy(img).float()\n",
    "        # gt_label = torch.from_numpy(gt_label).float()\n",
    "        output = {}\n",
    "        output = {'image': img, 'label': gt_label, 'filename': imgname}\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_id)\n",
    "\n",
    "    # def read_one(self, idx=None):\n",
    "    #     # if idx == None:\n",
    "    #     #     idx = np.random.randint(self.img_num)\n",
    "    #\n",
    "    #     # imgname, gt_label, imgidx = self.img_id[idx], self.label[idx], self.img_idx[idx]\n",
    "    #     imgname = self.img_id[idx]\n",
    "    #     gt_label = self.label[idx]\n",
    "    #     # imgidx = self.img_idx[idx]\n",
    "    #     imgpath = os.path.join(self.root_path, imgname)\n",
    "    #\n",
    "    #     try:\n",
    "    #         # img = PetrelHelper.pil_open(imgpath, \"RGB\")\n",
    "    #         img = cv2.imread(imgpath)\n",
    "    #         if self.transform is not None:\n",
    "    #             img = self.transform(img)\n",
    "    #\n",
    "    #         gt_label = gt_label.astype(np.float32)\n",
    "    #\n",
    "    #         output = {}\n",
    "    #         output = {'image': img, 'label': gt_label, 'filename': imgname}\n",
    "    #         return output\n",
    "    #     except:\n",
    "    #         print('{} load failed'.format(imgpath))\n",
    "    #         return self.read_one()\n"
   ],
   "id": "3e4abe2d22ca629c",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T08:16:47.934298Z",
     "start_time": "2025-02-24T08:16:47.706972Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# total_num = len(image_names)\n",
    "# num = 0\n",
    "# type_list = []\n",
    "# for i, image_name in enumerate(image_names):\n",
    "#     img_path = os.path.join(root_path, image_name)\n",
    "#     img = cv2.imread(img_path)\n",
    "#     label = labels[i]\n",
    "#     type_list.append(f'img_{type(img)}')\n",
    "#     type_list.append(f'label_{type(label)}')\n",
    "#     # if type(img) == None:\n",
    "#     #     print(image_name)\n",
    "#     #\n",
    "#     # if type(label) == None:\n",
    "#     #     print(image_name)\n",
    "#\n",
    "#     num+=1\n",
    "#\n",
    "# type_list = list(dict.fromkeys(type_list))\n",
    "# print(num)\n",
    "# print(type_list)\n",
    "\n",
    "dataset = AttrDataset(ginfo=None, augmentation=None, task_spec=None, train=True)\n",
    "DataLoader = data.DataLoader(dataset, batch_size=16, shuffle=True)"
   ],
   "id": "697b09e0615b0954",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T08:16:48.311861Z",
     "start_time": "2025-02-24T08:16:48.194308Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i, data in enumerate(DataLoader):\n",
    "    print(data)"
   ],
   "id": "aca548b957959cf5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14955\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'transform'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[49], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, data \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(DataLoader):\n\u001B[1;32m      2\u001B[0m     \u001B[38;5;28mprint\u001B[39m(data)\n",
      "File \u001B[0;32m/purestorage/AILAB/AI_2/youhans/miniconda3/envs/humanbench/lib/python3.10/site-packages/torch/utils/data/dataloader.py:633\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    630\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    631\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    632\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 633\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    634\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    635\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    636\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    637\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m/purestorage/AILAB/AI_2/youhans/miniconda3/envs/humanbench/lib/python3.10/site-packages/torch/utils/data/dataloader.py:677\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    675\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    676\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m--> 677\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    678\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m    679\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[0;32m/purestorage/AILAB/AI_2/youhans/miniconda3/envs/humanbench/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 51\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[0;32m/purestorage/AILAB/AI_2/youhans/miniconda3/envs/humanbench/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 51\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "Cell \u001B[0;32mIn[47], line 80\u001B[0m, in \u001B[0;36mAttrDataset.__getitem__\u001B[0;34m(self, index)\u001B[0m\n\u001B[1;32m     78\u001B[0m img \u001B[38;5;241m=\u001B[39m Image\u001B[38;5;241m.\u001B[39mopen(img_path)\n\u001B[1;32m     79\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m---> 80\u001B[0m     img \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     82\u001B[0m gt_label \u001B[38;5;241m=\u001B[39m gt_label\u001B[38;5;241m.\u001B[39mastype(np\u001B[38;5;241m.\u001B[39mfloat32)\n\u001B[1;32m     83\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[1;32m     84\u001B[0m \u001B[38;5;66;03m# img = torch.from_numpy(img).float()\u001B[39;00m\n\u001B[1;32m     85\u001B[0m \u001B[38;5;66;03m# gt_label = torch.from_numpy(gt_label).float()\u001B[39;00m\n",
      "File \u001B[0;32m/purestorage/AILAB/AI_2/youhans/workspace/reid/person/HumanBench/PATH/core/data/transforms/pedattr_transforms.py:319\u001B[0m, in \u001B[0;36mPedAttrRandomAugmentation.__call__\u001B[0;34m(self, img)\u001B[0m\n\u001B[1;32m    317\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, img):\n\u001B[1;32m    318\u001B[0m     img \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstandard_transform(img)\n\u001B[0;32m--> 319\u001B[0m     img \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrandom_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/purestorage/AILAB/AI_2/youhans/workspace/reid/person/HumanBench/PATH/core/data/transforms/pedattr_transforms.py:307\u001B[0m, in \u001B[0;36mRandAugment.__call__\u001B[0;34m(self, img)\u001B[0m\n\u001B[1;32m    305\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m op, minval, maxval \u001B[38;5;129;01min\u001B[39;00m ops:\n\u001B[1;32m    306\u001B[0m     val \u001B[38;5;241m=\u001B[39m (\u001B[38;5;28mfloat\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mm) \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m30\u001B[39m) \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mfloat\u001B[39m(maxval \u001B[38;5;241m-\u001B[39m minval) \u001B[38;5;241m+\u001B[39m minval\n\u001B[0;32m--> 307\u001B[0m     img \u001B[38;5;241m=\u001B[39m \u001B[43mop\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    309\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m img\n",
      "File \u001B[0;32m/purestorage/AILAB/AI_2/youhans/workspace/reid/person/HumanBench/PATH/core/data/transforms/pedattr_transforms.py:123\u001B[0m, in \u001B[0;36mTranslateYabs\u001B[0;34m(img, v)\u001B[0m\n\u001B[1;32m    121\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m random\u001B[38;5;241m.\u001B[39mrandom() \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0.5\u001B[39m:\n\u001B[1;32m    122\u001B[0m     v \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m-\u001B[39mv\n\u001B[0;32m--> 123\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mimg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m(img\u001B[38;5;241m.\u001B[39msize, PIL\u001B[38;5;241m.\u001B[39mImage\u001B[38;5;241m.\u001B[39mAFFINE, (\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m, v))\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'Tensor' object has no attribute 'transform'"
     ]
    }
   ],
   "execution_count": 49
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
